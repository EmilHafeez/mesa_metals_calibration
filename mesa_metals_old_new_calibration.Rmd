---
title: "MESA Metals 2: Examining Variability within Calibration Curves"
author: ""
geometry: "left=2.57cm,right=2.57cm,top=2.57cm,bottom=2.57cm"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage

# Section 1: Setup & Summary

Let's revisit the regression analysis and compare old and new calibrations, using the new (5/10/21) dataset. Mostly, this is to make sure there aren't systematic differences (between old and new calibrations), and then we can move into the lower range to determine values that were below the LOD. 

## Summary
TO EDITTO EDITTO EDITTO EDITTO EDITTO EDITTO EDIT
TO EDITTO EDITTO EDITTO EDITTO EDITTO EDITTO EDIT

Overall, the procedure is as follows:
    1. Run a regression for a single calibration and single data of a single chemical (arsenic chosen)
    2. Run a regression for all calibration runs of a single chemical (also arsenic)
    3. Run regressions of all runs on all chemicals. This step is likely of the most interest.
      1. Print plots of these regressions, and also print plots of the intercepts versus their slopes
      
We were primarily interested in replicating the regression, and then examining the variability between regression lines per each calibration run, as well as examining the slopes versus the intercepts (since this may be helpful later). See Section 6. 
      
## Questions

```{r, include = F}
library(tidyverse)
library(readxl)
library(modelr)
library(openxlsx)
```

```{r, include = F}
set.seed(1107)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  ggplot2.discrete.colour = "viridis",
  ggplot2.discrete.fill = "viridis"
)

knitr::opts_chunk$set(fig.align="center")

```

\newpage

# Section 2: Arsenic Case Study, Regression Results Replication Attempt

This section reads in just one elements calibration data (arsenic only). Then, we can fit a regression to this (the raw counts divided by internal standard, minus the calibration blank).

```{r, include = F}
#read in, basic tidy, iterate labels to make nesting possible, as in the first dataset
As_df = read_excel(
  "./data/210429_MESA totals calibration comparison_for Jeff and Emil.xlsx", 
  sheet = "As",
  range = "A2:E56") %>% 
  janitor::clean_names() %>% 
  rename(
    concentration_ug_old = concentration_ug_l_1,
    concentration_ug_new =  concentration_ug_l_4,
    net_signal_old = net_signal_counts_per_second_2,
    net_signal_new = net_signal_counts_per_second_5,
    no_cal_old = x3
  ) %>% 
  mutate(
    no_cal_new = NA
  ) %>% 
  mutate(
    run_old = rep(1:7, length.out = 54),
    run_new = rep(1:11, length.out = 54)
  ) %>% 
  mutate(
    no_cal_old = rep(1:8, each = 7, length.out = 54),
    no_cal_new = rep(1:8, each = 11, length.out = 54)
  ) %>% 
  select(no_cal_old, run_old, concentration_ug_old, net_signal_old, no_cal_new, run_new, concentration_ug_new, net_signal_new, everything())
```

```{r, include = F}
#test a case
As_df_test = 
  As_df %>% 
  filter(no_cal_old == 1)

glm_fit = glm(net_signal_old ~ concentration_ug_old, 
    data = As_df_test,
    family = gaussian(link = "identity")) 
```

Printing this regression output,

```{r}
summary(glm_fit)
```

It would be helpful to compare this result to what Kathrin and Rony produce using the spectroscope software.

# Section 3: Iterate Regression for All Arsenic

Let's iterate for each calibration number, for one element.
```{r, include = F}
#let's do both the old and the new results separately and then combine
#old
As_old_results_df =
  As_df %>%
  filter(!is.na(concentration_ug_old)) %>% 
  select(no_cal_old, run_old, concentration_ug_old, net_signal_old) %>% 
  nest(data = c(run_old, concentration_ug_old, net_signal_old)) %>%
  mutate(
    models =
      map(.x = data, ~glm(net_signal_old ~ concentration_ug_old, data = .x,
                     family = gaussian(link = "identity"))),
    results = map(models, broom::tidy)
  ) %>%
  select(no_cal_old, results) %>%
  unnest(results)
#new
As_new_results_df =
  As_df %>%
  filter(!is.na(concentration_ug_new)) %>% 
  select(no_cal_new, run_new, concentration_ug_new, net_signal_new) %>% 
  nest(data = c(run_new, concentration_ug_new, net_signal_new)) %>%
  mutate(
    models =
      map(.x = data, ~glm(net_signal_new ~ concentration_ug_new, data = .x,
                     family = gaussian(link = "identity"))),
    results = map(models, broom::tidy)
  ) %>%
  select(no_cal_new, results) %>%
  unnest(results)

#join
As_results_df = 
  bind_rows(As_old_results_df, As_new_results_df) %>% 
  mutate(
    old_new_identifier = rep(c("old_data", "new_data"), each = 14, length.out = 24)
  )
```

# Section 4: Plot Arsenic Regression Lines and Intercept vs Slope

Then we look at these regression lines. Each unique calibration run is its own regression line, as in the Excel structure. Here, we just do the older arsenic data (and then we'll look at all old and new data for all elements later).

```{r, echo = F}
As_df %>% 
  mutate(no_cal_old = as.factor(no_cal_old)) %>% 
  filter(!is.na(concentration_ug_old)) %>% 
ggplot(
  aes(
    x = concentration_ug_old, 
    y = net_signal_old, 
    group = no_cal_old, 
    color = no_cal_old,
    shape = no_cal_old)
  ) +
  stat_smooth(method = "lm", formula = y ~ x, se = F, size = .75) + 
  geom_point(size = 3) + 
    labs(color = "Regressions per Calibration",
         shape = "Regressions per Calibration",
         title = "Regressing Corrected Sample Concentration on Standard Concentration", 
         caption = "Examining the variability of arsenic as a case study") +
    scale_shape_manual(values=seq(0,7))
```

Focusing on these regression lines, let's take a look at the intercepts against the slopes. 
```{r, echo = F}
arsenic_results_df %>% 
  select(date, no_cal, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = c("estimate")) %>% 
  rename(intercept = c("(Intercept)")) %>% 
  rename(beta_hat = c("std_conc_ppb")) %>% 
  mutate(date = as.factor(date)) %>% 
ggplot(
  aes(
    x = intercept, 
    y = beta_hat, 
    group = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three"))), 
    color = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three"))),
    shape = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three"))))
  ) +
  geom_point(size = 3) + 
  scale_x_continuous(breaks = seq(-250, 50, 25)) +
  scale_y_continuous(breaks = seq(0, 15000, 1000)) +
    labs(color = "Regressions per \nDate.CalibrationNumber",
         shape = "Regressions per \nDate.CalibrationNumber",
         title = "Visualizing Intercepts vs Slopes", 
         caption = "Examining the variability of arsenic as a case study") +
    scale_shape_manual(values=seq(0,9))
```

# Section 5: All Elements Data Manipulation

Great. Now let's work on iterating for all elements (organized per Excel sheets within the workbook).

I read in the data.
```{r, include = F}
path =
  "./data/Calibration_check_MESA_04012021.xlsx"

mesa_metals_list = path %>%
  excel_sheets() %>%
  set_names() %>%
  map(read_excel, 
      path = path, 
      range = "A2:E38", 
      col_names = c("date","no_cal","std_conc_ppb","cps","cps_blk_corrected"))

#don't need the first and last sheet, "General" and "Distribution_sample_std"
mesa_metals_list = 
  mesa_metals_list[-c(1,19)]
```

Then, iterate the regressions for each of the (date,no_cal) groups within each element.
```{r, include = F}
mesa_metals_df = as_tibble(matrix(mesa_metals_list)) %>% 
  rename(elements_data = V1)

mesa_metals_df = 
  mesa_metals_df %>% 
  add_column(elements = c("cobalt_Co", "nickel_Ni", "zinc_Zn", "copper_Cu", "strontium_Sr", "molybdenum_Mo", "antimony_Sb", "cesium_Cs", "barium_Ba", "tungsten_W", "thallium_Tl", "lead_Pb", "uranium_U", "manganese_Mn", "selenium_Se", "arsenic_As", "cadmium_Cd")) %>% 
  select(elements, everything())

mesa_metals_df =
  mesa_metals_df %>%
  unnest(cols = elements_data) %>% 
  filter(!is.na(date))
```


# Section 6: Plot All Elements Regression Lines and Intercept vs Slope

Then, we're in a position to visualize the regression lines, and add the raw data to the plot just to see:

\pagestyle{fancy}
\fancyhf{}
\newpage
\paperwidth=\pdfpageheight
\paperheight=\pdfpagewidth
\pdfpageheight=\paperheight
\pdfpagewidth=\paperwidth
\headwidth=\textheight
\begingroup 
\vsize=\textwidth
\hsize=\textheight

```{r, message = F, warning = F, echo = F, fig.width= 25.37, fig.height = 19.02}
mesa_metals_df %>% 
  mutate(date = as.factor(date)) %>% 
ggplot(
  aes(
    x = std_conc_ppb, 
    y = cps_blk_corrected, 
    group = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three", "Cal_Four", "Cal_Five", "Cal_Six"))), 
    color = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three", "Cal_Four", "Cal_Five", "Cal_Six"))),
    shape = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three", "Cal_Four", "Cal_Five", "Cal_Six"))),
  )) +
  stat_smooth(method = "lm", formula = y ~ x, se = F, size = .75) +
  geom_point() + 
    labs(color = "Regressions per \nDate.CalibrationNumber",
         shape = "Regressions per \nDate.CalibrationNumber",
         title = "Regressing Corrected Blank Concentration on Standard Concentration", 
         subtitle = "Faceted by Elements", 
         caption = "Examining the variability between regression lines, by element, per date:calibration group") +
  facet_wrap(~elements, scales = "free") +
    scale_shape_manual(values=seq(0,9)) + 
  theme_bw(base_size = 30)
```

Interesting. There is some substantial variability between estimates within each element. Will think more on this. Note that thallium_Tl has a high value that is visible in the raw data (Excel too). 

Like we did for arsenic, let's look at the intercepts vs slopes for these regression lines, per element also. Note cobalt's additional calibration numbers.

```{r message=FALSE, warning=FALSE, include=F}
#make the results df
mesa_metals_results_df = 
  mesa_metals_df %>% 
  filter(!is.na(date)) %>% 
  nest(data = c(std_conc_ppb, cps, cps_blk_corrected)) %>%
  mutate(
    models =
      map(.x = data, ~glm(cps_blk_corrected ~ std_conc_ppb, data = .x,
                     family = gaussian(link = "identity"))),
    results = map(models, broom::tidy)
  ) %>%
  select(elements, date, no_cal, results) %>%
  unnest(results)
```

```{r, echo = F, message = F, warning = F, fig.fullwidth=TRUE}
#plot it
mesa_metals_results_df %>% 
  select(elements, date, no_cal, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = c("estimate")) %>% 
  rename(intercept = c("(Intercept)")) %>% 
  rename(beta_hat = c("std_conc_ppb")) %>% 
  mutate(date = as.factor(date)) %>% 
ggplot(
  aes(
    x = intercept, 
    y = beta_hat, 
    group = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three", "Cal_Four", "Cal_Five", "Cal_Six"))), 
    color = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three", "Cal_Four", "Cal_Five", "Cal_Six"))),
    shape = interaction(date,factor(no_cal, labels = c("Cal_One", "Cal_Two", "Cal_Three", "Cal_Four", "Cal_Five", "Cal_Six")))
    )
  ) +
  geom_point(size = 3) + 
    labs(color = "Regressions per \nDate.CalibrationNumber",
         shape = "Regressions per \nDate.CalibrationNumber",
         title = "Visualizing intercepts vs slopes", 
         subtitle = "Faceted by elements", 
         caption = "Examining the variability of intercepts vs slopes, by element, per date:calibration group") +
  facet_wrap(~elements, scales = "free") +
    scale_shape_manual(values=seq(15,23)) + 
  theme_bw(base_size = 8)
```

Interesting also. We're primarily interested in seeing these intercepts and slopes to see if one is correlated with the other, since this could be useful when we start extrapolating.

\endgroup
\newpage
\paperwidth=\pdfpageheight
\paperheight=\pdfpagewidth
\pdfpageheight=\paperheight
\pdfpagewidth=\paperwidth
\headwidth=\textwidth